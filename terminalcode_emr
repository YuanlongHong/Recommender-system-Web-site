•	列出作业：
yarn application -list
•	
•	杀死作业：
yarn application -kill Application-Id


•	下载代码和数据：
aws s3 sync s3://draco-emr-backup/code/ /home/hadoop/code/
aws s3 sync s3://draco-emr-backup/task/ /home/hadoop/task/

•	创建 HDFS 目录（如果没有）：
hdfs dfs -mkdir -p /data

•	Step 2：上传 ratings.csv 到 HDFS：
hdfs dfs -put /home/hadoop/task/ratings.csv /data/
•	Step 3：确认上传成功
hdfs dfs -ls /data


•	跑mr1:
Step 1：删除旧的 MR1 输出目录
hdfs dfs -rm -r -f /usercf_mr1
Step 2：跑 MR1（使用本地 mapper1.py / reducer1.py）：
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -files /home/hadoop/code/mr1/mapper1.py,/home/hadoop/code/mr1/reducer1.py \
    -D mapreduce.job.reduces=40 \
    -input /data/ratings_train.csv \
    -output /usercf_mr1 \
    -mapper mapper1.py \
-reducer reducer1.py

Step 3：查看 MR1 是否运行成功：
hdfs dfs -ls /usercf_mr1
查看 MR1 输出内容：
hdfs dfs -cat /usercf_mr1/part-* | head


•	跑mr2:
•	如果文件存在可以先删：
hdfs dfs -rm -r /usercf_mr2
•	运行代码：
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -files /home/hadoop/code/mr2/mapper2.py,/home/hadoop/code/mr2/reducer2.py \
    -D mapreduce.job.reduces=40 \
    -input /usercf_mr1 \
    -output /usercf_mr2 \
    -mapper mapper2.py \
-reducer reducer2.py

•	查看 MR2 是否运行成功：
hdfs dfs -ls /usercf_mr2
•	查看 MR2 输出内容：
hdfs dfs -cat /usercf_mr2/part-00000 | head


•	跑mr3:
•	运行代码：
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
  -D mapreduce.job.reduces=40 \
  -files /home/hadoop/code/mr3/mapper3.py,/home/hadoop/code/mr3/reducer3.py \
  -mapper mapper3.py \
  -reducer reducer3.py \
  -input /usercf_mr2 \
  -output /usercf_mr3




•	从s3直接把step1结果放到hdfs：
hdfs dfs -mkdir -p /user/hadoop/mr3_step1

hadoop distcp s3a://draco-emr-backup/output/mr3_step1/ /user/hadoop/mr3_step1



下载结果文件到s3：

# --- 1. 定义文件名映射 ---
# 我们把 HDFS 的文件夹名映射成好读的 .txt 文件名

# 1.1 User-Based MR2 (原始相似度)
echo "正在处理 User-Based MR2..."
hadoop fs -getmerge /usercf_mr2 user_mr2_raw.txt
aws s3 cp user_mr2_raw.txt s3://draco-emr-backup/output/

# 1.2 Item-Based MR2 (原始相似度)
echo "正在处理 Item-Based MR2..."
hadoop fs -getmerge /usercf_mr2_itembased item_mr2_raw.txt
aws s3 cp item_mr2_raw.txt s3://draco-emr-backup/output/

# 1.3 User-Based MR3 (最终 Top-K)
echo "正在处理 User-Based MR3..."
# 注意：如果你还没跑 MR3，这一步会报错，没关系
hadoop fs -getmerge /usercf_mr3 user_topk_neighbors.txt
aws s3 cp user_topk_neighbors.txt s3://draco-emr-backup/output/

# 1.4 Item-Based MR3 (最终 Top-K)
echo "正在处理 Item-Based MR3..."
hadoop fs -getmerge /usercf_mr3_itembased item_topk_neighbors.txt
aws s3 cp item_topk_neighbors.txt s3://draco-emr-backup/output/

echo "--------------------------------"
echo "所有文件处理完毕！请去 S3 Bucket (draco-emr-backup/output/) 查看。"
